{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employer Project: the Bank of England\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<p>\n",
    "<h3>\n",
    "\n",
    "[1. Introduction](#1.-Introduction)\n",
    "\n",
    "[2. Goals](#2.-Goals)\n",
    "\n",
    "[3. Data Collection & Preprocessing](#3.-Data-Collection-&-Preprocessing)\n",
    "\n",
    "[4. Data Analysis](#4.-Data-Analysis)\n",
    " \n",
    "[5. Appendix A: List of Files Needed for this Notebook](#Appendix-A:-List-of-Files-Needed-for-this-Notebook)\n",
    "\n",
    "[6. Appendix B: List of Accompanying Notebooks](#Appendix-B:-List-of-Accompanying-Notebooks)\n",
    "\n",
    "</h3>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The client for this project is the Bank of England. Here is an excerpt from the project briefing, profiling the Bank:\n",
    "\n",
    "> The Bank of England's mission is to promote the good of the people of the United Kingdom by maintaining monetary and financial stability. The Bank of England plays a multifaceted role in the national economy. Its primary objectives include maintaining price stability and supporting the government’s economic policies. To achieve this, the Bank has control over monetary policy instruments, primarily the setting of interest rates. By altering interest rates, the Bank can influence borrowing costs for businesses and individuals, which in turn affects spending, investment, and inflation. \n",
    "\n",
    "More information about the Bank of England can be found [here.](https://www.bankofengland.co.uk/about)\n",
    "\n",
    "\n",
    "The scenario for this project, as outlined in the briefing, is as follows: \n",
    "\n",
    "> Part of the job of the Bank of England is to provide reassurance and stability to financial markets. One way this is achieved is through representatives of the Bank delivering speeches at various public events. As an organisation, the Bank of England is interested in how the trends in these speeches correlate with observed events and economic indicators, as well as how the sentiment of these speeches can be used to predict market behaviour. This analysis will inform our understanding of the impact of the Bank’s communications on the economy, as well as the predictive power of using this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business Questions\n",
    "\n",
    "To provide insight into the above, The Bank of England's Data Strategy & Implementation Division needs answers to the following questions:\n",
    "\n",
    "1. Has the sentiment of central bank speeches changed over time? If so, how has it changed?\n",
    "<br>\n",
    "<br>\n",
    "2. How does the sentiment of the Bank of England’s speeches correlate with key events such as:\n",
    "    * bank rate decisions (including direction/magnitude of the change)\n",
    "    * publication of the Monetary Policy Report\n",
    "    * publication of the  Financial Stability Report/Review\n",
    "    * any other events or trends that may be relevant or interesting?  \n",
    "<br>\n",
    "<br>\n",
    "3. How does the sentiment of speeches correlate with key economic indicators of the UK, such as:\n",
    "    * GDP growth\n",
    "    * inflation\n",
    "    * labour market statistics (e.g. unemployment and wages)\n",
    "    * any other economic indicators that may be relevant or interesting.\n",
    "<br>   \n",
    "<br>\n",
    "4. Do these speeches have any predictive power to assist in predicting market behaviour?\n",
    "<br>\n",
    "<br>\n",
    "5.  Are there other insights or findings from the analysis that may be of interest to the organisation?\n",
    "<br>\n",
    "<br>\n",
    "6.  What are the potential reasons for any of the correlations discovered above? How have you drawn these conclusions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Goals\n",
    "\n",
    "\n",
    "**1. To submit a 'Project Scope and Plan' by 18th March:**\n",
    "* A 1,000-word overview and project plan covering project roles, roadmap, objectives, communications plans, work agreements, as well as a refined problem statement and draft project scope\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2. To submit an 'Initial Recommendation Pitch' by 15th April:**\n",
    "\n",
    "* Presentation deck (pdf) covering background/context, summary of analysis and visualisation approach, data-informed recommendations and conclusion\n",
    "* Presentation recording (5-10 mins) (MP4)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**3. To submit a 'Final Report and Presentation' by 22nd April:**\n",
    "\n",
    "* Pdf Report (1,500 words (+/- 10%) describing the problem, approach, insights identified; recommendations; all aspects of presentation\n",
    "* Code (file or link), submitted as private GitHub repo, Jupyter Notebook or RMarkdown (must be reproducible) \n",
    "* Presentation deck (pdf): a summary of the process followed by the group, visualised data story justifiying recommendations\n",
    "* Live presentation (10-15 mins) (date TBC)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**4. To submit an 'Individual Reflection' by 22nd April:** \n",
    "\n",
    "* Pdf document (500 words, +/- 10%) covering reflections on how effectively the group worked together, what challenging situations we encountered and how we responded to them, what contributions we made as individuals and what was the most useful feedback we received that we can use for group projects moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Data Collection & Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1 Getting the Data\n",
    "\n",
    "\n",
    "#### Data for Natural Language Processing\n",
    "\n",
    "* `all_speeches.csv`: this is a [publically-available Kaggle dataset](https://www.kaggle.com/datasets/davidgauthier/central-bank-speeches/data) comprised of a corpus of speeches from senior central bankers of various influential central banks. This corpus covers the period from 1997 until 2022 and was provided to the team as part of the project briefing\n",
    "<br>\n",
    "<br>\n",
    "* `LSE_DA_BoE_Employer_project_Sentiment-labelled_wordlist.xlsx`: a list of words labelled with sentiment, provided along with the project brief\n",
    "<br>\n",
    "<br>\n",
    "* `scraped_speeches.csv`: the most recent governor and deputy governor speeches scraped from the [Bank of England web pages.]( https://www.bankofengland.co.uk/news/speeches ) by the project team. For the code used for scraping and preprocessing of this data set, please see accompanying workbook `scraping_speeches.ippynb`\n",
    "\n",
    "\n",
    "#### Bank of England Data\n",
    "\n",
    "* `mpcvoting.xlsx`: a record of voting decisions by the Bank of England in relation to Bank Rate (6th Jun 1997-1st Feb 2024), Stock of Government Bond Purchases (4th Aug 2016-21st Sep 2023), Stock of Corporate Bond Purchases (4th Aug 2016-3rd Feb 2022) and Asset Purchase Decisions (5th Mar 2009-4th Aug 2016).  Each of the tabs was cleaned and preprocessed- see section 3.2.3 below\n",
    "<br>\n",
    "<br>\n",
    "* Links were also provided to publicly-available Bank of England reports: the ['Monetary Policy Reports'](https://www.bankofengland.co.uk/monetary-policy-report/monetary-policy-report)  and ['Financial Stability Reports'](https://www.bankofengland.co.uk/financial-stability-report/financial-stability-reports). Publication dates for these were scraped from the Bank of England web pages.The code for this is provided in a separate Notebook (`monetary_policy_reports_beautifulsoup.ipynb` and `financial_stability_reports_beautifulsoup.ipynb`) \n",
    "\n",
    "\n",
    "#### Additional Office for National Statistics (OfS) Data\n",
    "\n",
    "* Links to publicly-available data-sets related to [GDP Growth](https://www.ons.gov.uk/economy/grossdomesticproductgdp), [Inflation and Price Indices](https://www.ons.gov.uk/economy/inflationandpriceindices) and [Labour Market Statistics](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/bulletins/uklabourmarket/previousReleases) were provided as part of the briefing. The team chose to focus on the following: \n",
    "  * NOMIS Economic Activity Data\n",
    "  * ONS Vacancies Data\n",
    "  * ONS GDP Data\n",
    "  * ONS CPI and CPIH Monthly Indices\n",
    "  * ONS Average Weekly Earnings Data <div class=\"alert alert-block alert-info\">\n",
    "<b>@Alison:</b> Could you provide me the links to where you downloaded each of these datsets from please and I will add them here, thank you!\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2 Preprocessing the Data\n",
    "\n",
    "### 3.2.1 Preprocessing of Sentiment Analysis Data\n",
    "\n",
    "The following steps were followed:\n",
    "\n",
    "* `all_speeches.csv` and `scraped_speeches.csv` were combined into one dataset\n",
    "* This dataset was cleaned\n",
    "* The following sentiment analysis was carried out: \n",
    "  * Vader\n",
    "  * TextBlob\n",
    "  * Implementation of lexicon-based classifier based on the provided Loughran-McDonald word list\n",
    "  * Roberta model\n",
    "  \n",
    "The outputs from this analysis was combined into the dataset `uk_speeches_sentiments_processed_3.0.csv`. This preprocessed dataset is presented below. \n",
    "  \n",
    "Please see separate Notebooks containing the code for the above: \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Val:</b> Which is your notebook(s) do you think we should attach with the submission? I will add to the list in Appendix B\n",
    "</div>\n",
    "\n",
    "The metadata for this dataframe is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Column**             | **Description**                                                                                                                                |\n",
    "|:--- |:--- |\n",
    "| reference                 | Speech reference number  |\n",
    "| country                    | Country where speech was made                                                                                                                  |\n",
    "| date      | Date of speech                 |\n",
    "| title | Title of speech |\n",
    "| author         | Who wrote the speech     |\n",
    "| is_gov             | 1= The person making the speech was a governor; 0= was not a governor                                                                                            |\n",
    "| text               | The text of the speech          |\n",
    "| string_len              | Number of characters in speech|\n",
    "| formatted_text               | Formatted text- check with Val how |\n",
    "| vader_neg | Vader negative sentiment score |\n",
    "| vader_neu | Vader neutral sentiment score  |\n",
    "| vader_pos | Vader positive sentiment score  |\n",
    "| vader_compound | Vader compound sentiment score  |\n",
    "| textblob_polarity | TextBlob polarity score  |\n",
    "| textblob_subjectivity | Vader neutral sentiment score  |\n",
    "| lm_num_filtered_tokens | Filtered tokens for lm classifier.\n",
    "  |\n",
    "| lm_num_negative |  The columns prefixed by 'lm_num' contain the count of tokens per sentiment in the speech.This is the Lm negative sentiment score. Negative = words with bad connotations (e.g. \"indict\", \"abandon\", \"default\")  |\n",
    "| lm_negative_ratio | The columns suffixed by 'ratio' contain the count of tokens per sentiment in the speech divided by the total number of tokens in the speech. This is the Lm negative ratio  |\n",
    "| lm_num_positive | Lm positive sentiment score. Positive = words with good connotations (e.g. \"best\", \"accomplish\", \"innovativeness\") |\n",
    "| lm_positive_ratio | Lm positive ratio  |\n",
    "| lm_num_uncertainty | Lm uncertainty score. Uncertainity = words indicating imprecision (e.g. \"approximate\", \"almost\", \"contingency\")\n",
    "  |\n",
    "| lm_uncertainty_ratio | Lm uncertainty ratio  |\n",
    "| lm_num_litigious | Lm litigious score. Litigious = litigation-related words (e.g. \"claimant\", \"tort\", \"absolves\")\n",
    "  |\n",
    "| lm_litigious_ratio | Lm litigious ratio  |\n",
    "| lm_num_strong | Lm strong sentiment score. Strong modal = words expressing certainty of an action (e.g. \"always\", \"definitely\", \"never\")\n",
    "  |\n",
    "| lm_strong_ratio | Lm stregnth ratio  |\n",
    "| lm_num_weak | Lm weak sentiment score. Weak modal = words expressing uncertainty of an action (e.g. \"almost\", \"could\", \"might\")\n",
    "  |\n",
    "| lm_weak_ratio | Lm weak ratio  |\n",
    "| lm_num_constraining | Lm constraining sentiment score. Constraining = words related to constraints (e.g. \"required\", \"obligations\", \"commit\")  |\n",
    "| lm_constraining_ratio | Lm constraining ratio  |\n",
    "| lm_polarity | The lm_polarity column computes (lm_num_positive - lm_num_negative) / (lm_num_positive + lm_num_negative) |\n",
    "| lm_subjectivity | The lm_subjectivity column computes (lm_num_positive + lm_num_negative) / lm_num_filtered_tokens  |\n",
    "| dovish-hawkish-polarity| The dovish-hawkish-polarity column computes per speech: (sum Dovish - sum Hawkish) / (sum Dovish + sum Hawkish)  |\n",
    "| dovish-hawkish-subjectivity | The dovish-hawkish-subjectity column computes per speech: (sum Dovish + sum Hawkish) / (sum Dovish + sum Hawkish + sum Neutral)\n",
    " |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries that we will use\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Please note:</b> the imported file below is still version 1.0; we need to replace this with version 3.0 when Val has finalised this (code will not currently run as intended)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uk_speeches=pd.read_csv('uk_speeches_sentiments_processed_V1.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>is_gov</th>\n",
       "      <th>text</th>\n",
       "      <th>string_len</th>\n",
       "      <th>formatted_text</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>...</th>\n",
       "      <th>lm_num_strong</th>\n",
       "      <th>lm_strong_ratio</th>\n",
       "      <th>lm_num_weak</th>\n",
       "      <th>lm_weak_ratio</th>\n",
       "      <th>lm_num_constraining</th>\n",
       "      <th>lm_constraining_ratio</th>\n",
       "      <th>lm_polarity</th>\n",
       "      <th>lm_subjectivity</th>\n",
       "      <th>dovish-hawkish-polarity</th>\n",
       "      <th>dovish-hawkish-subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r980915a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>1998-09-15</td>\n",
       "      <td>Speech</td>\n",
       "      <td>george</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you, Chairman. I'm actually very pleased...</td>\n",
       "      <td>13731</td>\n",
       "      <td>thank you chairman im actually very pleased to...</td>\n",
       "      <td>0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>0.086387</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.564259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r981021b_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>1998-10-21</td>\n",
       "      <td>Britain in Europe</td>\n",
       "      <td>george</td>\n",
       "      <td>0</td>\n",
       "      <td>It's a great pleasure to be here in the beauti...</td>\n",
       "      <td>24263</td>\n",
       "      <td>its a great pleasure to be here in the beautif...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>-0.029586</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r981021a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>1998-10-21</td>\n",
       "      <td>Impact of the recent turbulence in internation...</td>\n",
       "      <td>king</td>\n",
       "      <td>1</td>\n",
       "      <td>Few industries have suffered more from volatil...</td>\n",
       "      <td>13678</td>\n",
       "      <td>few industries have suffered more from volatil...</td>\n",
       "      <td>0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>-0.297872</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r981101a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>1998-11-01</td>\n",
       "      <td>Economic policy, with and without forecasts</td>\n",
       "      <td>budd</td>\n",
       "      <td>0</td>\n",
       "      <td>My topic this evening is the use of forecasts ...</td>\n",
       "      <td>27679</td>\n",
       "      <td>my topic this evening is the use of forecasts ...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.006488</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r981101b_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>1998-11-01</td>\n",
       "      <td>Inflation targeting in practice: the UK experi...</td>\n",
       "      <td>vickers</td>\n",
       "      <td>0</td>\n",
       "      <td>Six years ago this week, sterling left the exc...</td>\n",
       "      <td>27693</td>\n",
       "      <td>six years ago this week sterling left the exch...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>-0.383562</td>\n",
       "      <td>0.058447</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.026583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      reference         country        date  \\\n",
       "0  r980915a_BOE  united kingdom  1998-09-15   \n",
       "1  r981021b_BOE  united kingdom  1998-10-21   \n",
       "2  r981021a_BOE  united kingdom  1998-10-21   \n",
       "3  r981101a_BOE  united kingdom  1998-11-01   \n",
       "4  r981101b_BOE  united kingdom  1998-11-01   \n",
       "\n",
       "                                               title   author  is_gov  \\\n",
       "0                                             Speech   george       0   \n",
       "1                                  Britain in Europe   george       0   \n",
       "2  Impact of the recent turbulence in internation...     king       1   \n",
       "3        Economic policy, with and without forecasts     budd       0   \n",
       "4  Inflation targeting in practice: the UK experi...  vickers       0   \n",
       "\n",
       "                                                text  string_len  \\\n",
       "0  Thank you, Chairman. I'm actually very pleased...       13731   \n",
       "1  It's a great pleasure to be here in the beauti...       24263   \n",
       "2  Few industries have suffered more from volatil...       13678   \n",
       "3  My topic this evening is the use of forecasts ...       27679   \n",
       "4  Six years ago this week, sterling left the exc...       27693   \n",
       "\n",
       "                                      formatted_text  vader_neg  ...  \\\n",
       "0  thank you chairman im actually very pleased to...      0.084  ...   \n",
       "1  its a great pleasure to be here in the beautif...      0.068  ...   \n",
       "2  few industries have suffered more from volatil...      0.078  ...   \n",
       "3  my topic this evening is the use of forecasts ...      0.051  ...   \n",
       "4  six years ago this week sterling left the exch...      0.068  ...   \n",
       "\n",
       "   lm_num_strong  lm_strong_ratio  lm_num_weak  lm_weak_ratio  \\\n",
       "0           20.0         0.017452         16.0       0.013962   \n",
       "1           39.0         0.018705         30.0       0.014388   \n",
       "2           11.0         0.009016         17.0       0.013934   \n",
       "3           31.0         0.013408         42.0       0.018166   \n",
       "4           19.0         0.007606         49.0       0.019616   \n",
       "\n",
       "   lm_num_constraining  lm_constraining_ratio  lm_polarity  lm_subjectivity  \\\n",
       "0                  2.0               0.001745    -0.212121         0.086387   \n",
       "1                 16.0               0.007674    -0.029586         0.081055   \n",
       "2                  5.0               0.004098    -0.297872         0.077049   \n",
       "3                 15.0               0.006488    -0.294118         0.051471   \n",
       "4                 12.0               0.004804    -0.383562         0.058447   \n",
       "\n",
       "   dovish-hawkish-polarity  dovish-hawkish-subjectivity  \n",
       "0                     -1.0                     0.564259  \n",
       "1                      1.0                     0.163910  \n",
       "2                      1.0                     0.560575  \n",
       "3                      0.0                     0.000000  \n",
       "4                     -1.0                     0.026583  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Selecting Which Sentiment Scores to Use to Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were intereseted in exploring which of the sentiment scores would be best to use for exploring the relationship with the economic and financial indicators. \n",
    "\n",
    "First, we computed summary scores per classifier: polarity multiplied by subjectivity\n",
    "\n",
    "For example, if a speech is for the most part neutral (80% neutral, 20% subjective), but the part that is subjective is 100% positive (or dovish), we have:\n",
    "\n",
    "* polarity = 1 (because 100% of the subjective part is positive)\n",
    "* subjectivity = 0.2 (only 20% of the speech is either positive or negative)\n",
    "* summary = 1 multiplied by 0.2 = 0.2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Please note:</b> this code is taken from Notebook 'Val_Exploratory_Data_Analysis_V1.0': it won't work here yet as that Notebook was based on 'uk_speeches_sentiments_processed_V3.0.csv' which I dont have yet\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = pd.read_csv('uk_speeches_sentiments_processed_V3.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp['vader_summary'] = df_sp.apply(lambda x: (x['vader_polarity'] * x['vader_subjectivity']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp['textblob_summary'] = df_sp.apply(lambda x: x['textblob_polarity'] * x['textblob_subjectivity'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp['dovish-hawkish_summary'] = df_sp.apply(lambda x: x['dovish-hawkish-polarity'] * x['dovish-hawkish-subjectivity'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp['lm_summary'] = df_sp.apply(lambda x: x['lm_polarity'] * x['lm_subjectivity'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values:\n",
    "df_sp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sense check correlations between summaries of the various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list of column names with 'summary' in the name\n",
    "col_summaries = [c for c in df_sp.columns if 'summary' in c]\n",
    "print (col_summaries)\n",
    "collist = col_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_small = df_sp[collist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap \n",
    "dataplot = sb.heatmap(df_sp_small.corr(), cmap=\"YlGnBu\", annot=True) \n",
    "\n",
    "# displaying heatmap \n",
    "mp.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to easily review correlations\n",
    "def get_top_correlations_blog(df, threshold=0.4):\n",
    "    \"\"\"\n",
    "    df: the dataframe to get correlations from\n",
    "    threshold: the maximum and minimum value to include for correlations. For eg, if this is 0.4, only pairs haveing a correlation coefficient greater than 0.4 or less than -0.4 will be included in the results. \n",
    "    \"\"\"\n",
    "    orig_corr = df.corr()\n",
    "    c = orig_corr.abs()\n",
    "\n",
    "    so = c.unstack()\n",
    "\n",
    "    print(\"|    Variable 1    |    Variable 2    | Correlation Coefficient    |\")\n",
    "    print(\"|------------------|------------------|----------------------------|\")\n",
    "    \n",
    "    i=0\n",
    "    pairs=set()\n",
    "    result = pd.DataFrame()\n",
    "    for index, value in so.sort_values(ascending=False).iteritems():\n",
    "        # Exclude duplicates and self-correlations\n",
    "        if value > threshold \\\n",
    "        and index[0] != index[1] \\\n",
    "        and (index[0], index[1]) not in pairs \\\n",
    "        and (index[1], index[0]) not in pairs:\n",
    "            \n",
    "            print(f'|    {index[0]}    |    {index[1]}    |    {orig_corr.loc[(index[0], index[1])]}    |')\n",
    "            result.loc[i, ['Variable 1', 'Variable 2', 'Correlation Coefficient']] = [index[0], index[1], orig_corr.loc[(index[0], index[1])]]\n",
    "            pairs.add((index[0], index[1]))\n",
    "            i+=1\n",
    "    return result.reset_index(drop=True).set_index(['Variable 1', 'Variable 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_correlations_blog(df_sp_small, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in collist:\n",
    "    plt.hist(df_sp[c])\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings on summary scores comparisons of the various classifiers:\n",
    "* Vader and LM are highly correlated\n",
    "* Vader and TextBlob are somewhat correlated\n",
    "* TextBlob and LM are somewhat correlated\n",
    "* Hawkish-dovish is not correlated to any of the other classifiers\n",
    "* Various distribution shapes for all classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sense check correlations between polarities of the various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list of column names with 'polarity' in the name\n",
    "col_polarities = [c for c in df_sp.columns if 'polarity' in c]\n",
    "print (col_polarities)\n",
    "collist = col_polarities\n",
    "\n",
    "df_sp_small = df_sp[collist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap \n",
    "dataplot = sb.heatmap(df_sp_small.corr(), cmap=\"YlGnBu\", annot=True) \n",
    "\n",
    "# displaying heatmap \n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_correlations_blog(df_sp_small, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in collist:\n",
    "    plt.hist(df_sp[c])\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings on polarity scores comparisons of the various classifiers\n",
    "* Vader and LM are strongly correlated\n",
    "* Vader and TextBlob are somewhat correlated\n",
    "* TextBlob and LM are somewhat correlated\n",
    "* Hawkish-dovish is not correlated to any other classifier\n",
    "* Hawkish-dovish has an unusual distribution\n",
    "* Modal value for Hawkish-dovish is 'just slightly dovish' (dovish = positive, hawkish = negative)\n",
    "* TextBlob hardly has any negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense checking correlations between subjectivity of the various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list of column names with 'subjectivity' in the name\n",
    "col_subjectivities = [c for c in df_sp.columns if 'subjectivity' in c]\n",
    "print (col_subjectivities)\n",
    "collist = col_subjectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_small = df_sp[collist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap \n",
    "dataplot = sb.heatmap(df_sp_small.corr(), cmap=\"YlGnBu\", annot=True) \n",
    "\n",
    "# displaying heatmap \n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_correlations_blog(df_sp_small, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in collist:\n",
    "    plt.hist(df_sp[c])\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings on polarity scores comparisons of the various classifiers\n",
    "* Vader and LM are somewhat correlated\n",
    "* Hawkish-dovish distribution is not normal\n",
    "* Modal value for Hawkish-dovish is 'fairly neutral' (subjectivity close to 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "* Vader, LM and TextBlob classifier scores reflect the tone of the speech (irrespective of the monetary stance)\n",
    "* Hawkish-dovish classifer scores reflect the monetary stance: from extremely dovish (1) to extremely hawkish (-1)\n",
    "\n",
    "The conclusion is that the following two scores should be used to correlate with economic indicators:\n",
    "* lm_summary\n",
    "* dovish-hawkish_summary\n",
    "\n",
    "(NB We may need to say a bit more about why we reached this conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Preprocessing of Bank of England Data\n",
    "\n",
    "In brief, the following steps were taken:\n",
    "\n",
    "* The 'Bank Rates Decisions' tab from `mpcvoting.xlsx` was imported and cleaned; voting intentions were calculated for each member voting; and ratio of 'Hawkish to Dovish' was calculated; and then a further column added calculating strength of the decision\n",
    "*  The ''Stock of govt. bond purchases' tab from `mpcvoting.xlsx` was imported, cleaned and voting intentions by date retained\n",
    "*  The 'Stock of corp. bond purchases' tab was imported, cleaned and voting intentions by date retained\n",
    "* Asset purchase decision from 'Asset Purchase Decisions' tab were imported and processed \n",
    "* All of these dateframes were then combined into one dataframe, `MPC_Processed.csv`\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Alison:</b> Could you check on the above (I'm note sure if I've summarised it very well, thanks)?\n",
    "</div>\n",
    "\n",
    "Please see the Notebook `BoE_MPCVoting_Preprocessing.ipynb` for all of the code for this preprocessing. \n",
    "\n",
    "Here is the metadata for this dataframe:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Alison:</b> There are a couple of gaps in the below, would you minding adding them, thank you!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Column**             | **Description**                                                                                                                                |\n",
    "|:--- |:--- |\n",
    "| MPC_MeetingDate                | The meeting date  |\n",
    "| MPC_PreviousRate | Rate set at previous meeting                                                                                                                 |\n",
    "| MPC_RateDecided | Rate set at the meeting                 |\n",
    "| MPC_RateDecision| Overall decision (as string): 'Increase', 'Stay' or 'Decrease' |\n",
    "| MPC_RateChange| Computed column showing difference from previous rate set|\n",
    "| MPC_VotedIncrease|Calculated column based on number of members who voted to increase the rate |\n",
    "| MPC_VotedStay | Calculated column based on number of members who voted to keep the rate the same    |\n",
    "|MPC_VotedDecrease  | Calculated column based on number of members who voted to decrease the rate|\n",
    "| MPC_PropVotedIncRate  | For each meeting proportion who voted to increase the rate. 1 = all voted increase, 0.5 = half voted increase, 0 = none vote increase|\n",
    "| MPC_PropVotedDecRate | For each meeting proportion who voted to decrease the rate. 1 = all voted decrease, 0.5 = half voted decrease, 0 = none vote decrease |\n",
    "|MPC_PropVotedStayRate | For each meeting proportion who voted to keep the rate the same ('stay'). 1 = all voted stay, 0.5 = half voted stay, 0 = none vote stay  |\n",
    "| MPC_DecisionStrength | Returns the relevant proportion so if decision is to increase returns PropVotedIncRate |\n",
    "| MPC_StockGovtBond| Description here |\n",
    "| MPC_StockCorpBond| Description here |\n",
    "| MPC_BondStock | Total Asset Purchases financed with central bank reserves (£bn)  |\n",
    "| MPC_PreviousBondStock |Total Asset Purchases financed with central bank reserves  at previous meeting (£bn)\n",
    "  |\n",
    "| MPC_BondStock_Change |  Difference in total from previous meeting (£bn)  |\n",
    "| MPC_QEDec | Description here  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc=pd.read_csv('MPC_Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Preprocessing ONS and Other Economic Indicators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, the following steps were taken:\n",
    "\n",
    "* The `NOMIS_Economic activity_Raw.xlsx` and `ONS_Vacancies_Raw.xlsx` were cleaned and preprocessed. As both of these datasets were for quarterly reporting periods, they were combined by data into the dataframe, `EcoQ_Processed.csv`, which is presented below\n",
    "<br>\n",
    "<br>\n",
    "* The `ONS_GDP_Raw.xlsx`, `ONS_CPI and CPIH_Monthly indices_Raw.xlsx`, `ONS_CPI and CPIH_Monthly indices_Raw.xlsx` and `ONS_Real AWE_Monthly_Raw.xlsx`. As these datasets were all reporting on monthly periods, they were combined into the dataframe ''EcoM_Processed.csv', which is presented below. \n",
    "\n",
    "Please see the Notebook `Economic_indicators_Preprocessing.ipynb` for all of the code for this preprocessing.\n",
    "\n",
    "Here is the metadata for these dataframes, first `EcoQ_Processed`:\n",
    "\n",
    "Here is the metadata for this dataframe:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Alison:</b> Could you kindly provide Descriptions for these two tables? Thank you!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Column**             | **Description**                                                                                                                                |\n",
    "|:--- |:--- |\n",
    "| 3mths_ending\t               | Description here  |\n",
    "| UnemploymentRate | Description here                                                                                                                  |\n",
    "| Vacancies(000s) | Description here                 |\n",
    "| Unemployed(000s)| Description here |\n",
    "| UnEmp/Vacancy| Description here    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, `EcoM_Processed`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Column**             | **Description**                                                                                                                                |\n",
    "|:--- |:--- |\n",
    "| MonthRefers\t               | Description here  |\n",
    "| GDP | Description here                                                                                                                  |\n",
    "| CPIH | Description here                 |\n",
    "| CPI| Description here |\n",
    "| AWE_Real_2015| Description here    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoQ_Processed = pd.read_csv('EcoQ_Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3mths_ending</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "      <th>Vacancies(000s)</th>\n",
       "      <th>Unemployed(000s)</th>\n",
       "      <th>UnEmp/Vacancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992-05-01</td>\n",
       "      <td>0.062139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992-06-01</td>\n",
       "      <td>0.061725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-07-01</td>\n",
       "      <td>0.062281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992-08-01</td>\n",
       "      <td>0.062580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992-09-01</td>\n",
       "      <td>0.062540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  3mths_ending  UnemploymentRate  Vacancies(000s)  Unemployed(000s)  \\\n",
       "0   1992-05-01          0.062139              NaN               NaN   \n",
       "1   1992-06-01          0.061725              NaN               NaN   \n",
       "2   1992-07-01          0.062281              NaN               NaN   \n",
       "3   1992-08-01          0.062580              NaN               NaN   \n",
       "4   1992-09-01          0.062540              NaN               NaN   \n",
       "\n",
       "   UnEmp/Vacancy  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EcoQ_Processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcoM_Processed = pd.read_csv('EcoM_Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MonthRefers</th>\n",
       "      <th>GDP</th>\n",
       "      <th>CPIH</th>\n",
       "      <th>CPI</th>\n",
       "      <th>AWE_Real_2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>63.3398</td>\n",
       "      <td>69.158</td>\n",
       "      <td>69.231</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>63.9959</td>\n",
       "      <td>69.308</td>\n",
       "      <td>69.399</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-03-01</td>\n",
       "      <td>64.0355</td>\n",
       "      <td>69.446</td>\n",
       "      <td>69.549</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-04-01</td>\n",
       "      <td>64.6273</td>\n",
       "      <td>69.743</td>\n",
       "      <td>69.805</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-01</td>\n",
       "      <td>64.1371</td>\n",
       "      <td>70.021</td>\n",
       "      <td>70.021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MonthRefers      GDP    CPIH     CPI  AWE_Real_2015\n",
       "0  1997-01-01  63.3398  69.158  69.231            NaN\n",
       "1  1997-02-01  63.9959  69.308  69.399            NaN\n",
       "2  1997-03-01  64.0355  69.446  69.549            NaN\n",
       "3  1997-04-01  64.6273  69.743  69.805            NaN\n",
       "4  1997-05-01  64.1371  70.021  70.021            NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EcoM_Processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we carry out the analysis to answer each of the business questions in turn, using the above joined datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1 Has The Sentiment Of Central Bank Speeches Changed Over Time? If So, How Has It Changed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 How Does The Sentiment Of The Bank Of England’s Speeches Correlate With Key Events?\n",
    "<br>\n",
    "<b>\n",
    "Events to consider:\n",
    "<br>  \n",
    "    \n",
    "* bank rate decisions (including direction/magnitude of the change)\n",
    "* publication of the Monetary Policy Report\n",
    "* publication of the Financial Stability Report/Review\n",
    "* any other events or trends that may be relevant or interesting? \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 How Does The Sentiment Of Speeches Correlate With Key Economic Indicators Of The UK?\n",
    "<br>\n",
    "<b>\n",
    "Indicators to consider:\n",
    "<br> \n",
    "\n",
    "* GDP growth\n",
    "* inflation\n",
    "* labour market statistics (e.g. unemployment and wages)\n",
    "* any other economic indicators that may be relevant or interesting.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Do These Speeches Have Any Predictive Power To Assist In Predicting Market Behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Are There Other Insights Or Findings From the Analysis That May Be of Interest To The Organisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 What Are The Potential Reasons For Any Of The Correlations Discovered Above? How Have You Drawn These Conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: List of Files Needed for this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the files that are needed to run this Notebook:\n",
    "\n",
    "* `uk_speeches_sentiments_processed_V1.0.csv`\n",
    "* `MPC_Processed.csv`\n",
    "* `EcoQ_Processed.csv`\n",
    "* `EcoM_Processed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: List of Accompanying Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `scraping_speeches.ippynb`\n",
    "* `monetary_policy_reports_beautifulsoup.ipynb`\n",
    "* `financial_stability_reports_beautifulsoup.ipynb`\n",
    "* `(Insert list of NLP preprocessing notebooks once confirmed by Val)`\n",
    "* `BoE_MPCVoting_Preprocessing.ipynb`\n",
    "* `Economic_indicators_Preprocessing.ipynb` \n",
    "\n",
    "(I will add a note explaining what each one is once we have a complete list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>    \n",
    "\n",
    "[4.1 Has The Sentiment Of Central Bank Speeches Changed Over Time? If So, How Has It Changed?](#4.1-Has-The-Sentiment-Of-Central-Bank-Speeches-Changed-Over-Time?-If-So,-How-Has-It-Changed?)\n",
    "    \n",
    "[4.2 How Does The Sentiment Of The Bank of England’s Speeches Correlate With Key Events?](#4.2-How-Does-The-Sentiment-Of-The-Bank-Of-England’s-Speeches-Correlate-With-Key-Events?)    \n",
    "    \n",
    "[4.3 How Does The Sentiment Of Speeches Correlate With Key Economic Indicators Of The UK?](#4.3-How-Does-The-Sentiment-Of-Speeches-Correlate-With-Key-Economic-Indicators-Of-The-UK?)\n",
    "    \n",
    "[4.4 Do These Speeches Have Any Predictive Power To Assist In Predicting Market Behaviour?](#4.4-Do-These-Speeches-Have-Any-Predictive-Power-To-Assist-In-Predicting-Market-Behaviour?)\n",
    "    \n",
    "[4.5 Are There Other Insights Or Findings From The Analysis That May Be Of Interest To The Organisation?](#4.5-Are-There-Other-Insights-Or-Findings-From-The-Analysis-That-May-Be-Of-Interest-To-The-Organisation?)  \n",
    " \n",
    "[4.6 What Are the Potential Reasons for Any of the Correlations Discovered Above? How Have You Drawn these Conclusions?](#4.6-What-are-the-potential-reasons-for-any-of-the-correlations-discovered-above?-How-have-you-drawn-these-conclusions?)\n",
    " </h4> \n",
    " </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
